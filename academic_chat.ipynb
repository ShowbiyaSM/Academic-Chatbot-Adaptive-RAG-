{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4c722dd",
   "metadata": {},
   "source": [
    "IMPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7d317f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\academic-chatbot\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "\n",
    "import traceback\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9d46f6",
   "metadata": {},
   "source": [
    "EMBEDDING MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88347ffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "Loading weights: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 103/103 [00:00<00:00, 409.74it/s, Materializing param=pooler.dense.weight]                             \n",
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    }
   ],
   "source": [
    "# Load embedding model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "def get_embeddings(chunks):\n",
    "    embeddings = model.encode(chunks, show_progress_bar=True)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452af7bd",
   "metadata": {},
   "source": [
    "PDF TEXT EXTRACTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2603883",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "import hashlib\n",
    "\n",
    "def pdf_fingerprint(text):\n",
    "    \"\"\"\n",
    "    Generate a hash for entire PDF content to detect duplicate PDFs\n",
    "    \"\"\"\n",
    "    cleaned_text = \" \".join(text.split())  # normalize whitespace\n",
    "    return hashlib.sha256(cleaned_text.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            page_text = page.extract_text()\n",
    "            if page_text:\n",
    "                text += page_text + \"\\n\"\n",
    "    return text\n",
    "\n",
    "def chunk_text(text, chunk_size=400, chunk_overlap=50):\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    chunks = splitter.split_text(text)\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6de1d5",
   "metadata": {},
   "source": [
    "FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fe926b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîπ VectorDB with Duplicate Removal\n",
    "\n",
    "import faiss\n",
    "import numpy as np\n",
    "import hashlib\n",
    "\n",
    "def chunk_hash(chunk):\n",
    "    \"\"\"\n",
    "    Compute a hash for a text chunk to detect duplicates.\n",
    "    \"\"\"\n",
    "    return hashlib.md5(chunk.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "def pdf_hash(text):\n",
    "    return hashlib.md5(text.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "\n",
    "uploaded_pdf_hashes = set()\n",
    "\n",
    "class VectorDB:\n",
    "    def __init__(self, embedding_dim):\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.index = faiss.IndexFlatL2(embedding_dim)\n",
    "        self.chunks = []           # Stores {\"text\": ..., \"metadata\": ...}\n",
    "        self.chunk_hashes = set()  # Track duplicates\n",
    "\n",
    "    def add_chunks(self, chunks, embeddings, metadata_list=None):\n",
    "        \"\"\"\n",
    "        Add chunks to the FAISS index, ignoring duplicates.\n",
    "        \"\"\"\n",
    "        if metadata_list is None:\n",
    "            metadata_list = [{} for _ in chunks]\n",
    "\n",
    "        new_chunks = []\n",
    "        new_embeddings = []\n",
    "\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            h = chunk_hash(chunk)\n",
    "            if h not in self.chunk_hashes:\n",
    "                self.chunk_hashes.add(h)\n",
    "                new_chunks.append(chunk)\n",
    "                new_embeddings.append(embeddings[i])\n",
    "                self.chunks.append({\n",
    "                    \"text\": chunk,\n",
    "                    \"metadata\": {\n",
    "                        **metadata_list[i],\n",
    "                        \"doc_id\": metadata_list[i].get(\"doc_id\", \"unknown\")\n",
    "                    }\n",
    "                })\n",
    "\n",
    "\n",
    "        # Add only new embeddings to FAISS\n",
    "        if new_embeddings:\n",
    "            self.index.add(np.array(new_embeddings).astype('float32'))\n",
    "\n",
    "        #print(f\"Added {len(new_chunks)} new chunks. Total chunks: {len(self.chunks)}\")\n",
    "        print(f\"[VectorDB] Indexed {len(new_chunks)} chunks (Total stored: {len(self.chunks)})\")\n",
    "\n",
    "    def retrieve(self, query_embedding, top_k=5):\n",
    "        \"\"\"\n",
    "        Retrieve top-k most similar chunks for a query embedding.\n",
    "        Returns:\n",
    "            - List of chunks (with text + metadata)\n",
    "            - Similarity scores\n",
    "        \"\"\"\n",
    "        if self.index.ntotal == 0:\n",
    "            return [], []\n",
    "\n",
    "        D, I = self.index.search(np.array([query_embedding]).astype('float32'), top_k)\n",
    "        results = [self.chunks[i] for i in I[0]]\n",
    "        scores = 1 - D.flatten()  # convert L2 distances to pseudo-similarity\n",
    "        return results, scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24f510e",
   "metadata": {},
   "source": [
    "JUDGE LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43bab71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 105/105 [00:00<00:00, 424.99it/s, Materializing param=classifier.weight]                                    \n",
      "BertForSequenceClassification LOAD REPORT from: cross-encoder/ms-marco-MiniLM-L-6-v2\n",
      "Key                          | Status     |  | \n",
      "-----------------------------+------------+--+-\n",
      "bert.embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    }
   ],
   "source": [
    "# Cross-Encoder Judge (lightweight & fast)\n",
    "JUDGE_MODEL = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"\n",
    "judge_model = CrossEncoder(JUDGE_MODEL)\n",
    "\n",
    "def llm_judge(query, chunks, threshold=0.6, min_good_chunks=2):\n",
    "    \"\"\"\n",
    "    Returns True if retrieved chunks are sufficient to answer the query\n",
    "    \"\"\"\n",
    "    if not chunks:\n",
    "        return False\n",
    "\n",
    "    pairs = [(query, chunk[\"text\"]) for chunk in chunks]\n",
    "    scores = judge_model.predict(pairs)\n",
    "\n",
    "    good_chunks = sum(score >= threshold for score in scores)\n",
    "    return good_chunks >= min_good_chunks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69226b1b",
   "metadata": {},
   "source": [
    "LOAD PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a9a8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pdfs(pdf_files, seen_pdf_hashes=None):\n",
    "    if seen_pdf_hashes is None:\n",
    "        seen_pdf_hashes = set()\n",
    "\n",
    "    all_chunks = []\n",
    "\n",
    "    for file in pdf_files:\n",
    "        text = extract_text_from_pdf(file.name)\n",
    "        h = pdf_hash(text)\n",
    "\n",
    "        if h in seen_pdf_hashes:\n",
    "            print(f\"‚ö†Ô∏è Skipping duplicate PDF: {file.name}\")\n",
    "            continue\n",
    "\n",
    "        seen_pdf_hashes.add(h)\n",
    "\n",
    "        chunks = chunk_text(text)\n",
    "        all_chunks.extend(chunks)\n",
    "\n",
    "        print(f\"‚úÖ {file.name} ‚Üí {len(chunks)} chunks\")\n",
    "\n",
    "    print(f\"\\nTOTAL NEW CHUNKS: {len(all_chunks)}\")\n",
    "    return all_chunks, seen_pdf_hashes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16aae238",
   "metadata": {},
   "source": [
    "QUERY CHECKER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2620671f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate in .\\.venv\\Lib\\site-packages (1.12.0)\n",
      "Requirement already satisfied: numpy>=1.17 in .\\.venv\\Lib\\site-packages (from accelerate) (2.4.1)\n",
      "Requirement already satisfied: packaging>=20.0 in .\\.venv\\Lib\\site-packages (from accelerate) (25.0)\n",
      "Requirement already satisfied: psutil in .\\.venv\\Lib\\site-packages (from accelerate) (7.2.2)\n",
      "Requirement already satisfied: pyyaml in .\\.venv\\Lib\\site-packages (from accelerate) (6.0.3)\n",
      "Requirement already satisfied: torch>=2.0.0 in .\\.venv\\Lib\\site-packages (from accelerate) (2.10.0)\n",
      "Requirement already satisfied: huggingface_hub>=0.21.0 in .\\.venv\\Lib\\site-packages (from accelerate) (1.3.5)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in .\\.venv\\Lib\\site-packages (from accelerate) (0.7.0)\n",
      "Requirement already satisfied: filelock in .\\.venv\\Lib\\site-packages (from huggingface_hub>=0.21.0->accelerate) (3.20.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in .\\.venv\\Lib\\site-packages (from huggingface_hub>=0.21.0->accelerate) (2026.1.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in .\\.venv\\Lib\\site-packages (from huggingface_hub>=0.21.0->accelerate) (1.2.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in .\\.venv\\Lib\\site-packages (from huggingface_hub>=0.21.0->accelerate) (0.28.1)\n",
      "Requirement already satisfied: shellingham in .\\.venv\\Lib\\site-packages (from huggingface_hub>=0.21.0->accelerate) (1.5.4)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in .\\.venv\\Lib\\site-packages (from huggingface_hub>=0.21.0->accelerate) (4.67.2)\n",
      "Requirement already satisfied: typer-slim in .\\.venv\\Lib\\site-packages (from huggingface_hub>=0.21.0->accelerate) (0.21.1)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in .\\.venv\\Lib\\site-packages (from huggingface_hub>=0.21.0->accelerate) (4.15.0)\n",
      "Requirement already satisfied: anyio in .\\.venv\\Lib\\site-packages (from httpx<1,>=0.23.0->huggingface_hub>=0.21.0->accelerate) (4.12.1)\n",
      "Requirement already satisfied: certifi in .\\.venv\\Lib\\site-packages (from httpx<1,>=0.23.0->huggingface_hub>=0.21.0->accelerate) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in .\\.venv\\Lib\\site-packages (from httpx<1,>=0.23.0->huggingface_hub>=0.21.0->accelerate) (1.0.9)\n",
      "Requirement already satisfied: idna in .\\.venv\\Lib\\site-packages (from httpx<1,>=0.23.0->huggingface_hub>=0.21.0->accelerate) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in .\\.venv\\Lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface_hub>=0.21.0->accelerate) (0.16.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in .\\.venv\\Lib\\site-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in .\\.venv\\Lib\\site-packages (from torch>=2.0.0->accelerate) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in .\\.venv\\Lib\\site-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: setuptools in .\\.venv\\Lib\\site-packages (from torch>=2.0.0->accelerate) (80.10.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in .\\.venv\\Lib\\site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: colorama in .\\.venv\\Lib\\site-packages (from tqdm>=4.42.1->huggingface_hub>=0.21.0->accelerate) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in .\\.venv\\Lib\\site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.3)\n",
      "Requirement already satisfied: click>=8.0.0 in .\\.venv\\Lib\\site-packages (from typer-slim->huggingface_hub>=0.21.0->accelerate) (8.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9d3dc74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in .\\.venv\\Lib\\site-packages (1.2.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pip install python-dotenv\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6adf6ead",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 105/105 [00:00<00:00, 480.37it/s, Materializing param=classifier.weight]                                    \n",
      "BertForSequenceClassification LOAD REPORT from: cross-encoder/ms-marco-MiniLM-L-6-v2\n",
      "Key                          | Status     |  | \n",
      "-----------------------------+------------+--+-\n",
      "bert.embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "LOW_TH = 0.25\n",
    "HIGH_TH = 0.45\n",
    "TOP_K = 5\n",
    "MAX_RETRIEVER_ATTEMPTS = 3\n",
    "\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "base_url= os.getenv(\"BASE_URL\")\n",
    "from openai import OpenAI\n",
    "client = OpenAI(\n",
    "    api_key=api_key,\n",
    "    base_url=base_url\n",
    ")\n",
    "\n",
    "def llm(prompt):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4.1-nano\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.2\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "\n",
    "def generate_answer(query, retrieved_chunks):\n",
    "    \"\"\"\n",
    "    Generate answer using retrieved PDF chunks.\n",
    "    \"\"\"\n",
    "    context = \"\\n\\n\".join(\n",
    "        c[\"text\"] if isinstance(c, dict) else c\n",
    "        for c in retrieved_chunks\n",
    "    )\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    You are an academic assistant.\n",
    "    Answer ONLY using the provided context.\n",
    "    If the answer is not found, say \"I don't know\".\n",
    "\n",
    "    CONTEXT:\n",
    "    {context}\n",
    "\n",
    "    QUESTION:\n",
    "    {query}\n",
    "\n",
    "    ANSWER:\n",
    "    \"\"\"\n",
    "\n",
    "    response=llm(prompt)\n",
    "    return response.strip()\n",
    "\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Cross-Encoder Judge for evaluation\n",
    "# -----------------------------\n",
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "judge_model = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
    "\n",
    "def hallucination_check(answer, retrieved_chunks):\n",
    "    \"\"\"\n",
    "    True  -> hallucinated\n",
    "    False -> grounded\n",
    "    \"\"\"\n",
    "    context = \"\\n\".join(\n",
    "        f\"- {chunk['text']}\" for chunk in retrieved_chunks\n",
    "    )\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are a strict fact checker.\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "ANSWER:\n",
    "{answer}\n",
    "\n",
    "Question:\n",
    "Is the answer fully supported by the context?\n",
    "Reply with only YES or NO.\n",
    "\"\"\"\n",
    "\n",
    "    verdict = llm(prompt).strip().upper()\n",
    "    print(\"Hallucination verdict:\", verdict)\n",
    "\n",
    "    return verdict == \"NO\"\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "def correctness_check(answer, query):\n",
    "    \"\"\"\n",
    "    Uses LLM to judge if the answer correctly answers the query.\n",
    "\n",
    "    Returns:\n",
    "    - True  ‚Üí LOW correctness (needs regeneration)\n",
    "    - False ‚Üí GOOD correctness\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    You are a strict evaluator.\n",
    "\n",
    "    Question:\n",
    "    {query}\n",
    "\n",
    "    Answer:\n",
    "    {answer}\n",
    "\n",
    "    Is the answer correct and complete?\n",
    "\n",
    "    Respond with only one word:\n",
    "    YES or NO\n",
    "    \"\"\"\n",
    "\n",
    "    response = llm(prompt).strip().upper()\n",
    "\n",
    "    print(\"[Correctness Judge - LLM]:\", response)\n",
    "\n",
    "    return response != \"YES\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def adaptive_router(pdf_files, query):\n",
    "    try:\n",
    "        if not pdf_files:\n",
    "            return {\"Error\": \"No PDF uploaded\"}\n",
    "\n",
    "        # 1Ô∏è‚É£ Load PDFs\n",
    "        all_chunks = load_pdfs(pdf_files)\n",
    "        if not all_chunks:\n",
    "            return {\"Error\": \"No text extracted\"}\n",
    "\n",
    "        # 2Ô∏è‚É£ Embeddings\n",
    "        chunk_embeddings = get_embeddings(all_chunks)\n",
    "        query_embedding = get_embeddings([query])[0]\n",
    "\n",
    "        # 3Ô∏è‚É£ FAISS Indexing\n",
    "        vector_db = VectorDB(embedding_dim=len(chunk_embeddings[0]))\n",
    "        vector_db.add_chunks(all_chunks, chunk_embeddings)\n",
    "\n",
    "        retrieved_chunks, scores = vector_db.retrieve(query_embedding, top_k=TOP_K)\n",
    "        max_sim = float(max(scores))\n",
    "\n",
    "        # 4Ô∏è‚É£ Routing logic\n",
    "        stage = \"FAISS Similarity\"\n",
    "        retriever_attempts = 0\n",
    "        answer = \"\"\n",
    "\n",
    "        if max_sim < LOW_TH:\n",
    "            route = \"üß† General LLM (Low similarity)\"\n",
    "            answer = llm(query)\n",
    "            is_hallucinated = False\n",
    "            is_low_correctness = False\n",
    "\n",
    "\n",
    "        elif max_sim > HIGH_TH:\n",
    "            route = \"üìÑ PDF-based RAG (High similarity)\"\n",
    "            retrieved_chunks, _ = vector_db.retrieve(\n",
    "                query_embedding, top_k=len(all_chunks)\n",
    "            )\n",
    "            retriever_attempts = len(retrieved_chunks)\n",
    "            answer = generate_answer(query, retrieved_chunks)\n",
    "            is_hallucinated = hallucination_check(answer, retrieved_chunks)\n",
    "\n",
    "            # üîπ Check correctness\n",
    "            is_low_correctness = correctness_check(answer, query)\n",
    "\n",
    "            # üîπ Regenerate if needed\n",
    "            max_attempts = 3\n",
    "            attempt = 0\n",
    "            while (is_hallucinated or is_low_correctness) and attempt < max_attempts:\n",
    "                attempt += 1\n",
    "                \n",
    "                # If hallucinated ‚Üí try retrieving more chunks (expand top-k)\n",
    "                if is_hallucinated:\n",
    "                    retrieved_chunks, _ = vector_db.retrieve(query_embedding, top_k=len(retrieved_chunks)+TOP_K)\n",
    "\n",
    "                # Regenerate answer\n",
    "                answer = generate_answer(query, retrieved_chunks)\n",
    "\n",
    "                # Re-check\n",
    "                is_hallucinated = hallucination_check(answer, retrieved_chunks)\n",
    "                is_low_correctness = correctness_check(answer, query)\n",
    "\n",
    "        else:\n",
    "            # 5Ô∏è‚É£ Retriever + Judge\n",
    "            stage = \"Retriever + Judge\"\n",
    "            judge_decision = False\n",
    "            final_chunks = []\n",
    "\n",
    "            for attempt in range(MAX_RETRIEVER_ATTEMPTS):\n",
    "                k = TOP_K * (attempt + 1)\n",
    "                candidate_chunks, _ = vector_db.retrieve(query_embedding, top_k=k)\n",
    "\n",
    "                if llm_judge(query, [c[\"text\"] for c in candidate_chunks]):\n",
    "                    judge_decision = True\n",
    "                    final_chunks = candidate_chunks\n",
    "                    retriever_attempts = len(candidate_chunks)\n",
    "                    break\n",
    "\n",
    "            if judge_decision:\n",
    "                route = \"üìÑ PDF-based RAG (Judge confirmed)\"\n",
    "                answer = generate_answer(query, final_chunks)\n",
    "                is_hallucinated = hallucination_check(answer, final_chunks)\n",
    "                is_low_correctness = correctness_check(answer, query)\n",
    "\n",
    "            else:\n",
    "                route = \"üß† General LLM (Judge rejected)\"\n",
    "                answer = llm(query)\n",
    "\n",
    "        # üî• THIS RETURN CONTROLS GRADIO OUTPUT\n",
    "        return {\n",
    "            \"Routing Decision\": route,\n",
    "            \"Decision Stage\": stage,\n",
    "            \"Max Cosine Similarity\": round(max_sim, 3),\n",
    "            \"Retrieved Chunks\": retriever_attempts,\n",
    "            \"Answer\": answer,\n",
    "            \"Hallucinated\": is_hallucinated,\n",
    "            \"Low Correctness\": is_low_correctness\n",
    "        }\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"Error\": str(e)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e121e74f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hallucination verdict: YES\n",
      "Hallucinated (OK): False\n",
      "Hallucination verdict: NO\n",
      "Hallucinated (BAD): True\n"
     ]
    }
   ],
   "source": [
    "fake_chunks = [\n",
    "    {\"text\": \"Python is a programming language created by Guido van Rossum.\"},\n",
    "    {\"text\": \"Python was first released in 1991.\"}\n",
    "]\n",
    "\n",
    "answer_ok = \"Python was created by Guido van Rossum.\"\n",
    "answer_bad = \"Python was created by Elon Musk.\"\n",
    "\n",
    "print(\"Hallucinated (OK):\", hallucination_check(answer_ok, fake_chunks))\n",
    "print(\"Hallucinated (BAD):\", hallucination_check(answer_bad, fake_chunks))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d60cfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Correctness Judge - LLM]: YES\n",
      "Low correctness (GOOD): False\n",
      "[Correctness Judge - LLM]: NO\n",
      "Low correctness (BAD): True\n"
     ]
    }
   ],
   "source": [
    "query = \"Who created Python?\"\n",
    "\n",
    "answer_good = \"Python was created by Guido van Rossum.\"\n",
    "answer_bad = \"Python is a popular programming language.\"\n",
    "\n",
    "print(\"Low correctness (GOOD):\", correctness_check(answer_good, query))\n",
    "print(\"Low correctness (BAD):\", correctness_check(answer_bad, query))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d858c8c",
   "metadata": {},
   "source": [
    "GRADIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583e55fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\showbiya\\AppData\\Local\\Temp\\gradio\\4489c96c65070de787d873107aece9179117ef1b02e0adf9472b40e004c6f5e3\\22IS601-LM-1.1.pdf ‚Üí 42 chunks\n",
      "\n",
      "TOTAL CHUNKS: 42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00,  2.02it/s]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 36.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VectorDB] Indexed 42 chunks (Total stored: 42)\n",
      "Hallucination verdict: YES\n",
      "[Correctness Judge - LLM]: NO\n",
      "C:\\Users\\showbiya\\AppData\\Local\\Temp\\gradio\\4489c96c65070de787d873107aece9179117ef1b02e0adf9472b40e004c6f5e3\\22IS601-LM-1.1.pdf ‚Üí 42 chunks\n",
      "\n",
      "TOTAL CHUNKS: 42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00,  2.54it/s]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 10.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VectorDB] Indexed 42 chunks (Total stored: 42)\n",
      "C:\\Users\\showbiya\\AppData\\Local\\Temp\\gradio\\4489c96c65070de787d873107aece9179117ef1b02e0adf9472b40e004c6f5e3\\22IS601-LM-1.1.pdf ‚Üí 42 chunks\n",
      "\n",
      "TOTAL CHUNKS: 42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:02<00:00,  1.17s/it]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VectorDB] Indexed 42 chunks (Total stored: 42)\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "gr.Interface(\n",
    "    fn=adaptive_router,\n",
    "    inputs=[\n",
    "        gr.File(file_types=[\".pdf\"], file_count=\"multiple\", label=\"Upload PDFs\"),\n",
    "        gr.Textbox(label=\"Ask a question\")\n",
    "    ],\n",
    "    outputs=gr.JSON(label=\"Routing Result\"),\n",
    "    title=\"Adaptive RAG (Notebook Mode)\",\n",
    "    description=\"Cosine similarity + Retriever Grader + Judge\"\n",
    ").launch()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa1def1",
   "metadata": {},
   "source": [
    "``\n",
    "PDF ‚Üí Chunks\n",
    "     ‚Üí Embeddings\n",
    "     ‚Üí FAISS Index\n",
    "Query ‚Üí Embedding\n",
    "      ‚Üí FAISS search\n",
    "      ‚Üí Threshold routing\n",
    "      ‚Üí (Optional) Retriever + Judge loop\n",
    "      ‚Üí Final decision\n",
    "\n",
    "``"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca99bfa",
   "metadata": {},
   "source": [
    "# üîπ Adaptive RAG Flow (Query Checker + Retriever Grader)\n",
    "\n",
    "## **1Ô∏è‚É£ Query Checker (Router)**\n",
    "\n",
    "**Goal:** Determine if the user query is related to the uploaded PDFs.  \n",
    "\n",
    "**Flow:**\n",
    "\n",
    "1. User uploads PDFs ‚Üí **extract text** ‚Üí **split into chunks** ‚Üí **generate embeddings** ‚Üí **store in FAISS**.\n",
    "2. User enters query ‚Üí **convert query to embedding**.\n",
    "3. **FAISS search:** Compare query embedding against all PDF chunks.\n",
    "4. **Decision based on similarity thresholds:**\n",
    "   - **High similarity** ‚Üí PDF-based RAG  \n",
    "   - **Low similarity** ‚Üí General LLM  \n",
    "   - **Ambiguous similarity** ‚Üí **Cross-Encoder Judge** checks if the query is actually answerable from PDFs.\n",
    "\n",
    "**Summary:**  \n",
    "- Uses **FAISS** for similarity search.  \n",
    "- Uses **Judge** only for ambiguous cases.  \n",
    "\n",
    "---\n",
    "\n",
    "## **2Ô∏è‚É£ Retriever Checker (Grader Loop)**\n",
    "\n",
    "**Goal:** Ensure that the retrieved PDF chunks are sufficient to answer the query.  \n",
    "\n",
    "**Flow:**\n",
    "\n",
    "1. Take **top-k chunks** from FAISS (from query checker) ‚Üí **candidate chunks**.\n",
    "2. **Judge each batch** to check if they are actually relevant to the query.\n",
    "3. If not enough relevant chunks ‚Üí retrieve **next batch of top-k chunks** from FAISS (repeat up to 3 attempts max).\n",
    "4. Decide:\n",
    "   - **Judge confirmed** ‚Üí use these chunks for PDF-based RAG  \n",
    "   - **Judge rejected** ‚Üí fallback to General LLM\n",
    "\n",
    "**Summary:**  \n",
    "- Uses **FAISS** to retrieve candidate chunks.  \n",
    "- Uses **Judge** to validate relevance and sufficiency.  \n",
    "\n",
    "\n",
    "Analogy\n",
    "\n",
    "Think of it like a library system:\n",
    "FAISS = just the shelf positions of the books (embeddings and indices).\n",
    "VectorDB = shelf positions + actual books + catalog info (it keeps the text and metadata).\n",
    "\n",
    "When you search:\n",
    "\n",
    "- Query ‚Üí embedding ‚Üí FAISS ‚Üí returns indices of nearest embeddings.\n",
    "- VectorDB ‚Üí uses these indices ‚Üí returns actual text chunks so you can pass them to the LLM judge.\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
